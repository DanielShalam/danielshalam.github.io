<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Daniel Shalam</title>
  <style>
    body {font-family:'Inter',sans-serif;background:#fafafa;color:#111;line-height:1.6;max-width:900px;margin:auto;padding:40px;}
    header {display:flex;flex-direction:column;align-items:center;text-align:center;margin-bottom:40px;}
    .avatar {width:220px;height:220px;border-radius:50%;object-fit:cover;margin-bottom:20px;border:3px solid #ccc;}
    nav a {color:#2F80ED;text-decoration:none;margin:0 6px;}
    .btn {color:#2F80ED;background:#fff;border:1px solid #ddd;padding:6px 10px;border-radius:6px;text-decoration:none;font-size:0.9rem;}
    .pubimg {width:100%;height:180px;object-fit:cover;border-radius:10px;margin-bottom:10px;border:1px solid #ddd;background:#f2f2f2;}
    .pub {margin-bottom:30px;background:#fff;padding:15px;border-radius:10px;box-shadow:0 1px 3px rgba(0,0,0,.06);}
    details.btn {background:#fff;border:1px solid #ddd;border-radius:6px;padding:6px 10px;margin-top:6px;}
  </style>
</head>
<body>
  <header>
    <img src="main.jpg" alt="Daniel Shalam" class="avatar" />
    <h1>Daniel Shalam</h1>
    <p>Ph.D. Candidate, Computer Science – University of Haifa</p>
    <p>I am a Ph.D. candidate in Computer Science at the University of Haifa, advised by Dr. Simon Korman. My research focuses on representation learning, multimodal alignment, and generative modeling - studying how independent vision and text encoders can be aligned for few-shot multimodal learning. My work combines principles from optimal transport, flow-based methods, and data-efficient adaptation. I received my M.Sc. from the University of Haifa, where I developed optimal-transport-based frameworks for few-shot classification and self-supervised learning. I am a recipient of the Bloom Excellence Scholarship and the Dean’s List Award, and presented my research at the Israel Computer Vision Day 2024. I am broadly interested in efficient and interpretable learning systems that bridge theoretical insight with large-scale experimentation.</p>
    <nav>
      <a href="#publications">Publications</a>
      <a href="CV_Daniel.pdf">CV</a>
      <a href="mailto:Dani360@gmail.com">Email</a>
      <a href="https://scholar.google.com/citations?user=Ffu2TCkAAAAJ" target="_blank">Scholar</a>
      <a href="https://github.com/DanielShalam" target="_blank">GitHub</a>
    </nav>
  </header>

  <section id="publications">
    <h2>Publications</h2>

    <div class="pub">
      <div class="venue">Under review</div>
      <div class="title"><strong>Few-Shot Flow: Flow-Matching Alignment for Uni-Modal Vision-Language Models</strong></div>
      <div class="authors"><strong>Daniel Shalam</strong>, Simon Korman</div>
      <img class="pubimg" src="FSF_workflow.png" alt="Few-Shot Flow paper image"/>
      <details class="btn"><summary>Abstract</summary><p>Few-Shot Flow (FSF) aligns independent vision and text encoders without paired data. It combines an orthogonal Procrustes mapping with a flow-matching prior that preserves geometry while enabling flexible adaptation for few-shot multimodal classification, achieving strong results on challenging domains such as medical imaging.</p></details>
      <div class="buttons">
        <a class="btn" href="#">arXiv (soon)</a>
      </div>
    </div>

    <div class="pub">
      <div class="venue">ECCV 2024</div>
      <div class="title"><strong>Unsupervised Representation Learning by Balanced Self-Attention Matching (BAM)</strong></div>
      <div class="authors"><strong>Daniel Shalam</strong>, Simon Korman</div>
      <img class="pubimg" src="BAM_workflow.png" alt="BAM paper image"/>
      <details class="btn"><summary>Abstract</summary><p>Balanced Self-Attention Matching (BAM) aligns self-attention representations across images to learn geometry-preserving, semantically consistent features in a completely unsupervised manner. This approach improves downstream recognition and robustness to distribution shifts.</p></details>
      <div class="buttons">
        <a class="btn" href="https://arxiv.org/abs/2408.02014" target="_blank">Paper</a>
        <a class="btn" href="https://github.com/DanielShalam/BAM" target="_blank">Code</a>
      </div>
    </div>

    <div class="pub">
      <div class="venue">ICML 2024</div>
      <div class="title"><strong>The Balanced-Pairwise-Affinities Feature Transform (BPA)</strong></div>
      <div class="authors">Daniel Shalam, Simon Korman</div>
      <img class="pubimg" src="bpa_workflow.png" alt="BPA paper image"/>
      <details class="btn"><summary>Abstract</summary><p>The Balanced-Pairwise-Affinities (BPA) transform is a training-free optimal-transport-inspired feature transform that balances pairwise relations among samples, improving transductive few-shot classification without retraining and achieving state-of-the-art results on multiple benchmarks.</p></details>
      <div class="buttons">
        <a class="btn" href="https://proceedings.mlr.press/v235/shalam24a.html" target="_blank">Paper</a>
        <a class="btn" href="https://github.com/DanielShalam/BPA" target="_blank">Code</a>
      </div>
    </div>

    <div class="pub">
      <div class="venue">BMVC 2023</div>
      <div class="title"><strong>MFSC: Matching by Few-Shot Classification</strong></div>
      <div class="authors">Daniel Shalam, Elie Abboud, Roee Litman, Simon Korman</div>
      <img class="pubimg" src="MFSC_workflow.png" alt="MFSC paper image"/>
      <details class="btn"><summary>Abstract</summary><p>MFSC reinterprets visual correspondence as a few-shot classification task, where match candidates act as class prototypes. This approach enhances performance under low supervision and domain shifts, demonstrating competitive results on multiple correspondence benchmarks.</p></details>
      <div class="buttons">
        <a class="btn" href="https://papers.bmvc2023.org/0677.pdf" target="_blank">Paper</a>
      </div>
    </div>
  </section>
</body>
</html>
